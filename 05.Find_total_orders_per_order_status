#Find total orders per order status
#Tried to implement this use case using various Key transformations available

#Transformations used in this use-case: Map, Join, CountByKey, GroupByKey, ReduceByKey, CombineByKey
#Actions used in this use-case: ForEach

####################### Using scala #############################################
val ordersRDD = sc.textFile("/home/cloudera/orders.txt")
val orderItemsRDD = sc.textFile("/home/cloudera/order_items.txt")

#creating tuple object with order_id as key and order_status as value
val ordersMap = ordersRDD.map(x => x.split(",")).map(x => (x(0).toInt, x(3)))

#creating tuple object with order_id as key and order_product_id as value
val orderItemsMap = orderItemsRDD.map(x => x.split(",")).map(x => (x(1).toInt, x(2).toInt))

#joining orders with orderitems
val ordersJoinedMap = orderItemsMap.join(ordersMap)

#creating tuple object by having order_status as key and 1 as value
val orderStatusMap = ordersJoinedMap.map(x => (x._2._2, 1))

#Using countByKey to find total orders for each order_status
orderStatusMap.countByKey.foreach(println)

#Using groupByKey to find total orders for each order_status
orderStatusMap.groupByKey().map(x => (x._1, x._2.sum)).foreach(println)

#Using reduceByKey to find total orders for each order_status
orderStatusMap.reduceByKey((a, b) => a + b).foreach(println)

#Using combineByKey to find total orders for each order_status.
#With combineByKey we can define the initial value for each accumulator
orderStatusMap.combineByKey(value => 1, (acc: Int, value) => acc + value, (acc1: Int, acc2: Int) => acc1 + acc2).foreach(println)

####################### Using spark-sql (SQLContext) ############################
import org.apache.spark.sql.{SQLContext, Row}

val sqlContext = new SQLContext(sc)

sqlContext.sql("set spark.sql.shuffle.partitions=10")

case class Orders(order_id: Int, order_status: String)

case class OrderItems(order_item_order_id: Int, order_item_subtotal: Float)

val ordersMap = sc.textFile("/home/cloudera/orders.txt").map(x => x.split(",")).map(x => Orders(x(0).toInt, x(3)))

val orderItemsMap = sc.textFile("/home/cloudera/order_items.txt").map(x => x.split(",")).map(x => OrderItems(x(1).toInt, x(4).toFloat))

import sqlContext.createSchemaRDD

ordersMap.registerTempTable("OrdersTbl")

orderItemsMap.registerTempTable("OrderItemsTbl")

val totalOrdersPerOrderStatus = sqlContext.sql("select o.order_status, count(1) from OrdersTbl o join OrderItemsTbl oi on o.order_id=oi.order_item_order_id group by o.order_status order by o.order_status")

totalOrdersPerOrderStatus.collect.foreach(println)

####################### Using Hive (HiveContext) ####################################
#Pre-requisites: Table must be exists in Hive
import org.apache.spark.sql.hive.HiveContext

val sqlContext = new HiveContext(sc)

sqlContext.sql("set spark.sql.shuffle.partitions=10")

sqlContext.sql("use retail_db")

val totalOrdersPerOrderStatus = sqlContext.sql("select o.order_status, count(1) from orders o join order_items oi on o.order_id=oi.order_item_order_id group by o.order_status order by o.order_status")

totalOrdersPerOrderStatus.collect.foreach(println)

#Validation Script (Using Sqoop Eval)
sqoop eval \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--query "select o.order_status, count(1) from orders o join order_items oi on o.order_id=oi.order_item_order_id group by o.order_status order by o.order_status"
